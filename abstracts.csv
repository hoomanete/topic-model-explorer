year,name,content
2016,Rawal2016a,"Across all industry segments, 96 percent of systems could be breached on average. In the game of cyber security, every moment a new player (attacker) is entering the game with new skill sets. An attacker only needs to be effective once while defenders of cyberspace have to be successful all of the time. There will be a first-mover advantage in such a chasing game, which means that the first move often wins. In this paper, in order to face the security challenges brought in by attacker's first move advantage, we analyzed the past ten years of cyber-attacks, studied the immediate attack's pattern and offer the tools to predict the next move of the cyber attacker.
"
2007,Fette2007,"Each month, more attacks are launched with the aim of making web users believe that they are communicating with a trusted entity for the purpose of stealing account information, logon credentials, and identity information in general. This attack method, commonly known as “phishing,” is most commonly initiated by sending out emails with links to spoofed websites that harvest information. We present a method for detecting these attacks, which in its most general form is an application of machine learning on a feature set designed to highlight user-targeted deception in electronic communication. This method is applicable, with slight modification, to detection of phishing websites, or the emails used to direct victims to these sites. We evaluate this method on a set of approximately 860 such phishing emails, and 6950 non-phishing emails, and correctly identify over 96% of the phishing emails while only mis-classifying on the order of 0.1% of the legitimate emails. We conclude with thoughts on the future for such techniques to specifically identify deception, specifically with respect to the evolutionary nature of the attacks and information available."
2006,Saple2006,"Network fault management is concerned with the detection, isolation, and correction of anomalous conditions that occur in a computer network. Present state of art in fault management classifies existing methodologies into two main categories: reactive rule based approaches and intelligent monitoring systems. In this paper we explore the concept of anticipatory behavior to develop an intelligent agent-based network management model, which uses an anticipatory agent to proactively detect occurrence of faults using a predictive model pertaining to network performance. To compare the effectiveness of the anticipatory technique, we build a simulation model of a network using the DEVS framework. Two reactive rule based fault management strategies are compared against the anticipatory approach. Results of the comparative analysis are presented to demonstrate the potential of the anticipatory technique in detecting network anomalies.
"
2016,Chang2016c,"An important part of an organisation's mission is protecting its information assets from inside or outside threats. As the information environment has become more diverse and inclusive, security concern has shifted from information assets resided in the organisation to information assets and networked devices exposed to broader cyberspace, such as cloud or Internet of things environment and mobile Internet. Organisations have to keep up with the knowledge and trends in information security and cybersecurity to safeguard their information assets. Knowledge mapping will aid in this sort of knowledge management process. Mandatory standards and government regulations help industries establish best practices in cybersecurity. Knowledge mapping and scientometric analysis across disciplines also provide a tracking system to notify researchers and practitioners should the new solutions and technology facilitating threat detection emerge. While various topics in information security and cybersecurity have been extensively investigated in academia, identifying salient themes and development trajectories in information security and cybersecurity research is relatively unexplored. This study employs scientometric analysis and topic modelling to develop knowledge maps that visualise core concepts associated with information security and cybersecurity research over time and across disciplines. With scientometric analysis and knowledge mapping using topic models, this study identifies the commonality, difference, and relationship between information security and cybersecurity research domains. This approach could gain insights into how these research areas have evolved and might be improved concerning learning and teaching cybersecurity. The proposed approach to developing the knowledge map may be extended to other research areas. The nature of cybersecurity research is interdisciplinary (Craigen et al., 2014). Learning the findings and methodologies used in information and cybersecurity studies from multiple disciplines is conducive to our understanding of mutual.
"
2015,Karl2015,"Text analytics continue to proliferate as mass volumes of unstructured but highly useful data are generated at unbounded rates. Vector space models for text data - in which documents are represented by rows and words by columns - provide a translation of this unstructured data into a format that may be analyzed with statistical and machine learning techniques. This approach gives excellent results in revealing common themes, clustering documents, clustering words, and in translating unstructured text fields (such as an open-ended survey response) to usable input variables for predictive modeling. After discussing the collection and processing of text, we explore properties and transformations of the document-term matrix (DTM). We show how the singular value decomposition may be used to drastically reduce the size of the document space while also setting the stage for automatic topic extraction, courtesy of the varimax rotation. This latent semantic analysis (LSA) approach produces factors that are compatible with graphical exploration and advanced analytics. We also explore Latent Dirichlet Allocation for topic analysis. We reference published R packages to implement the methods and conclude with a summary of other popular open-source and commercial software packages.
"
2016,Weiss2016,"“ This is a needle in a haystack problem where ” the appearance of the needle is unknown. Conducting a literature review in new domains presents unique challenges. The literature in a new domain is typically broad, fragmented, and growing quickly. Because little is known about the new domain, the literature review cannot be guided by established classifications of knowledge, unlike in an existing domain. Rather, it will be driven by evidence that challenges and extends existing knowledge. In a way, exploring a new domain means looking for anomalies in the evidence that cannot be explained by what is already known. This article summarizes lessons from conducting two literature reviews in new domains in the area of cybersecurity. It then presents a design for using leader-driven crowdsourcing to collect evidence and synthesize it into insights in a new domain. The article will be relevant to those who are exploring a new domain, in particular students, researchers, and members of R&D projects in industry.
"
2013,Schoemaker2013,"As firms become more networked they greatly expand their points of contact with the outside world. This can greatly help the detection of early signs of threats or opportunities emerging at the periphery. But a major challenge for firms scanning the periphery of their networks is how to manage the explosion of information. How do they avoid undue distraction while spotting useful signals amid an avalanche of data? We discuss how strategic radars can be used to integrate outside networks, weak signals, sense making, strategic dialog and scenario planning. A brief case study illustrates how a strategic radar system was actually developed and deployed by a large government agency in order to enhance its adaptive capability for coping with increasing external change.
"
2014,LiHorkoff2014,"Security has been a growing concern for most large organizations, especially financial and government institutions, as security breaches in the socio-technical systems they depend on are costing billions. A major reason for these breaches is that socio-technical systems are designed in a piecemeal rather than a holistic fashion that leaves parts of a system vulnerable. To tackle this problem, we propose a three-layer security analysis framework for socio-technical systems involving business processes, applications and physical infrastructure. In our proposal, global security requirements lead to local security requirements that cut across layers and upper-layer security analysis influences analysis at lower layers. Moreover, we propose a set of analytical methods and a systematic process that together drive security requirements analysis throughout the three-layer framework. Our proposal supports analysts who are not security experts by defining transformation rules that guide the corresponding analysis. We use a smart grid example to illustrate our approach. Keyword: Security Requirements Goal Model Multilayer SocioTechnical System Security Pattern
"
2010,Neuhaus2010b,"-We study the vulnerability reports in the Common Vulnerability and Exposures (CVE) database by using topic models on their description texts to find prevalent vulnerability types and new trends semi-automatically. In our study of the 39,393 unique CVEs until the end of 2009, we identify the following trends, given here in the form of a weather forecast: PHP: declining, with occasional SQL injection. Buffer Overflows: flattening out after decline. Format Strings: in steep decline. SQL Injection and XSS: remaining strong, and rising. Cross-Site Request Forgery: a sleeping giant perhaps, stirring. Application Servers: rising steeply.
"
2015,Rader2015,"Computer users have access to computer security information from many different sources, but few people receive explicit computer security training. Despite this lack of formal education, users regularly make many important security decisions, such as “Should I click on this potentially shady link?” or “Should I enter my password into this form?” For these decisions, much knowledge comes from incidental and informal learning. To better understand differences in the security-related information available to users for such learning, we compared three informal sources of computer security information: news articles, web pages containing computer security advice, and stories about the experiences of friends and family. Using a Latent Dirichlet Allocation topic model, we found that security information from peers usually focuses on who conducts attacks, information containing expertise focuses instead on how attacks are conducted, and information from the news focuses on the consequences of attacks. These differences may prevent users from understanding the persistence and frequency of seemingly mundane threats (viruses, phishing), or from associating protective measures with the generalized threats the users are concerned about (hackers). Our findings highlight the potential for sources of informal security education to create patterns in user knowledge that affect their ability to make good security decisions.
"
2015,Toomela2015,"Anticipation is an inevitable characteristic of life. Instead of asking whether this or that organism reveals some form of anticipation as it is often done in biology and psychology today, it is more fruitful to ask in which ways different organisms anticipate future. In this chapter Anokhin's Functional Systems Theory is taken as a starting point to proceed with the analysis of how psychic and cultural mechanisms of anticipation have evolved over the history of mind. Grounded also on Vygotsky's and Lotman's theories, it is concluded that there are nine different developmentally ordered mechanisms of thought and correspondingly nine different forms of anticipation. Knowing the basic mechanisms of thinking, it becomes possible to evaluate research in anticipation from a new perspective. Limitations of less developed forms of anticipation can be recognized and replaced with more efficient hierarchically higher-order forms of anticipatory thinking.
"
2017,Ruan2017,"This is the first in a series of papers on the risk measures and unifying economic framework encompassing the cross-disciplinary field of “Cybernomics”.This is also the first academic paper to formally propose measurement units for cyber risk. In this paper, multidisciplinary methodologies are used to apply proven risk measurement methods in finance and medicine to define novel risk units central to cybernomics. Leveraging established risk units - MicroMort (MM) for measuring medical risk and Value-at-Risk (VaR) for measuring market risk - BitMort (BM) and hekla (named after an Icelandic volcano) are defined as cyber risk units. Risk calculation methods and examples are introduced in this paper to measure costeffectiveness of control factors, articulate an entity's “willingness-to-pay” (risk pricing) for cyber risk reduction, cyber risk limit, and cyber risk appetite. Built around BM and hekla, cybernomics integrates cyber risk management and economics to study the requirements of a databank in order to improve risk analytics solutions for: 1) the valuation of digital assets; 2) the measurement of risk exposure of digital assets; and 3) the capital optimization for managing residual cyber risk. Establishing adequate, holistic and statistically robust data points on the entity, portfolio and global levels for the development of a cybernomics databank are essential for the resilience of our shared digital future. This paper explains the need to establish data schemes such as International Digital Asset Classification (IDAC) and International Classification of Cyber Incidents (ICCI).
"
2014,He2014,"Attack-defense models play an important role in cybersecurity systems’ design. After reviewing traditional and prevailing attack-defense models, the authors discuss recently proposed paradigm shifts and how to adopt new models."
2017,Millar2017,"According to Veracode, a Gartner-recognised leader in application security, 44% of applications contain critical vulnerabilities in an open source component [16]. Most companies do not have a reliable way of being notified when zero-day vulnerabilities1 are found, or when patches are made available. This means that attack vectors in Open Source Software (OSS) exist longer than they should. This paper discusses the cause of OSS vulnerabilities, why they are a major issue, and how they may be mitigated. Conventional methods of detection are discussed along with novel approaches and research trends. A new conclusion is made that it may not be possible to replace expert human inspection of OSS although it can be effectively augmented with techniques such as machine learning, IDE plug-ins and repository linking to make OSS implementation and review less time intensive. Underpinning any technological advances should be better knowledge at the human level – development teams need trained, coached and improved so they can implement OSS more securely, know what vulnerabilities to look for and how to handle them. It is the use of this blended approach to detection which is key."
2010,Li2010,"Alert correlation techniques effectively improve the quality of alerts reported by intrusion detection systems, and are sufficient to support rapid identification of ongoing attacks or predict an intruder's next likely goal. In our previous work, an alert correlation approach based on our XSWRL ontology has been proposed. This paper focuses on how to develop the intrusion alerts correlation system according to our alert correlation approach. At first, the multi-agent system architecture consisting of agents and sensors is shown. The sensors collect security relevant information, and the agents process the information. Then we present each modules of the system in detail. The State Sensor collects information about security state and the Local State Agent and Center State Agent preprocess the security state information and convert it to ontology. The Attack Sensor collects information about attack and the Local Alert Agent and Center Alert Agent preprocess the alert information and convert it to ontology. The Attack Correlator correlates the attacks and outputs the attack sessions.
"
2016,Badalkhani2016,"This research examines the use of an anticipatory method and publicly available newsworthy information on ten past cyber failures of critical infrastructures in the United States to predict cyber failures of networks and systems of technology startups.
The Anticipatory Failure Determination (AFD) method was modified to enable the use of publicly available information. A list of the resources that were used to cause the ten cyber failures in critical infrastructures was produced and used to make predictions of failure scenarios of a stack of open source software. Finally, the factors that enable and constrain the use of the AFD method to predict cyber failures in technology startups were specified.
Junior engineers, designers, contractors and other stakeholders of cyber systems as well as government policy makers will be interested in outcome of this research to predict potential cyber failures for proactive mitigation."
2017,Mathew2017b,"-Researchers should be able to explore software engineering, however their whims guide them. But do our current structure of conferences and journals inhibits that free exploration? To check this, this paper explores the trends in software engineering research within 35,391 Software Engineering (SE) papers from 34 leading SE venues over the last 25 years. These trends are discovered via a combination of (a) text mining; (b) Latent Dirichlet Allocation, to find the topics; (c) search-based software engineering, to automatically tune LDA; (d) clustering conferences and journals according to how often they publish on each topic. Trends in these topics reveal what topics are becoming more/less popular over time. Overlaps and gaps between conferences and journals are identified, from which we can identify what journals/ conferences might be merged and what new venues might be created to cover gaps. These results can be used for strategic and tactical purposes. Organizers of SE venues could use them as a long-term planning aid for improving and rationalizing how they service our research community. Also, individual researchers could also use these results to make short-term publication plans. Note that our analysis is 100% automatic, thus making it readily repeatable and easily updatable.
"
2017,Jain2017,"Nearly all modern software has security flaws-either known or unknown by the users. However, metrics for evaluating software security (or lack thereof) are noisy at best. Common evaluation methods include counting the past vulnerabilities of the program, or comparing the size of the Trusted Computing Base (TCB), measured in lines of code (LoC) or binary size. Other than deleting large swaths of code from project, it is dificult to assess whether a code change decreased the likelihood of a future security vulnerability. Developers need a practical, constructive way of evaluating security. This position paper argues that we actually have all the tools needed to design a better, empirical method of security evaluation. We discuss related work that estimates the severity and vulnerability of certain attack vectors based on code properties that can be determined via static analysis. This paper proposes a grand, unified model that can predict the risk and severity of vulnerabilities in a program. Our prediction model uses machine learning to correlate these code features of open-source applications with the history of vulnerabilities reported in the CVE (Common Vulnerabilities and Exposures) database. Based on this model, one can incorporate an analysis into the standard development cycle that predicts whether the code is becoming more or less prone to vulnerabilities.
"
2016,Pasquale2016,"The increase in crimes targeting the cloud is increasing the amount of data that must be analysed during a digital forensic investigation, exacerbating the problem of processing such data in a timely manner. Since collecting all possible evidence proactively could be cumbersome to analyse, evidence collection should mainly focus on gathering the data necessary to investigate potential security breaches that can exploit vulnerabilities present in a particular cloud configuration. Cloud elasticity can also change the attack surface available to an adversary and, consequently, the way potential security breaches can arise. Therefore, evidence collection should be adapted depending on changes in the cloud configuration, such as those determined by allocation/deallocation of virtual machines. In this paper, we propose to use attack scenarios to configure more effective evidence collection for cloud services. In particular, evidence collection activities are targeted to detect potential attack scenarios that can violate existing security policies. These activities also adapt when new/ different attack scenarios can take place due to changes in the cloud configuration. We illustrate our approach by using examples of insider and outsider attacks. Our results demonstrate that using attack scenarios allows us to target evidence collection activities towards those security breaches that are likely, while saving space and time necessary to store and process such data.
"
2008,Rieck2008,"Malicious software in form of Internet worms, computer viruses, and Trojan horses poses a major threat to the security of networked systems. The diversity and amount of its variants severely undermine the effectiveness of classical signature-based detection. Yet variants of malware families share typical behavioral patterns reflecting its origin and purpose. We aim to exploit these shared patterns for classification of malware and propose a method for learning and discrimination of malware behavior. Our method proceeds in three stages: (a) behavior of collected malware is monitored in a sandbox environment, (b) based on a corpus of malware labeled by an anti-virus scanner a malware behavior classifier is trained using learning techniques and (c) discriminative features of the behavior models are ranked for explanation of classification decisions. Experiments with different heterogeneous test data collected over several months using honeypots demonstrate the effectiveness of our method, especially in detecting novel instances of malware families previously not recognized by commercial anti-virus software.
"
2016,Case2016,"On December 23, 2015, the Ukrainian Kyivoblenergo, a regional electricity distribution company, reported service outages to customers. The outages were due to a third party’s illegal entry into the company’s computer and SCADA systems: Starting at approximately 3:35 p.m. local time, seven 110 kV and 23 35 kV substations were disconnected for three hours. Later statements indicated that the cyber attack impacted additional portions of the distribution grid and forced operators to switch to manual mode. The event was elaborated on by the Ukrainian news media, who conducted interviews and determined that a foreign attacker remotely controlled the SCADA distribution management system. The outages were originally thought to have affected approximately 80,000 customers, based on the Kyivoblenergo’s update to customers. However, later it was revealed that three different distribution oblenergos (a term used to describe an energy company) were attacked, resulting in several outages that caused approximately 225,000 customers to lose power across various areas.
Shortly after the attack, Ukrainian government officials claimed the outages were caused by a cyber attack, and that Russian security services were responsible for the incidents. Following these claims, investigators in Ukraine, as well as private companies and the U.S. government, performed analysis and offered assistance to determine the root cause of the outage. Both the E-ISAC and SANS ICS team was involved in various efforts and analyses in relation to this case since December 25, 2015, working with trusted members and organizations in the community.
This joint report consolidates the open source information, clarifying important details surrounding the attack, offering lessons learned, and recommending approaches to help the ICS community repel similar attacks. This report does not focus on attribution of the attack."
2014,Scandariato2014,"-This paper presents an approach based on machine learning to predict which components of a software application contain security vulnerabilities. The approach is based on text mining the source code of the components. Namely, each component is characterized as a series of terms contained in its source code, with the associated frequencies. These features are used to forecast whether each component is likely to contain vulnerabilities. In an exploratory validation with 20 Android applications, we discovered that a dependable prediction model can be built. Such model could be useful to prioritize the validation activities, e.g., to identify the components needing special scrutiny.
"
2017,Do2017,"In this survey, we review the existing game-theoretic approaches for cyber security and privacy issues, categorizing their application into two classes, security and privacy. To show how game theory is utilized in cyberspace security and privacy, we select research regarding three main applications: cyber-physical security, communication security, and privacy. We present game models, features, and solutions of the selected works and describe their advantages and limitations from design to implementation of the defense mechanisms. We also identify some emerging trends and topics for future research. This survey not only demonstrates how to employ game-theoretic approaches to security and privacy but also encourages researchers to employ game theory to establish a comprehensive understanding of emerging security and privacy problems in cyberspace and potential solutions.
"
2017,Rass2017,"Advanced persistent threats (APT) combine a variety of different attack forms ranging from social engineering to technical exploits. The diversity and usual stealthiness of APT turns them into a central problem of contemporary practical system security, since information on attacks, the current system status or the attacker's incentives is often vague, uncertain and in many cases even unavailable. Game theory is a natural approach to model the conflict between the attacker and the defender, and this work investigates a generalized class of matrix games as a risk mitigation tool for an advanced persistent threat (APT) defense. Unlike standard game and decision theory, our model is tailored to capture and handle the full uncertainty that is immanent to APTs, such as disagreement among qualitative expert risk assessments, unknown adversarial incentives and uncertainty about the current system state (in terms of how deeply the attacker may have penetrated into the system's protective shells already). Practically, game-theoretic APT models can be derived straightforwardly from topological vulnerability analysis, together with risk assessments as they are done in common risk management standards like the ISO 31000 family. Theoretically, these models come with different properties than classical game theoretic models, whose technical solution presented in this work may be of independent interest.
"
2015,Ferguson2015,"Data characterization, trending, correlation, and sense making are almost always performed after the data is collected. As a result, big-data analysis is an inherently forensic (after-the-fact) process. In order for network defenders to be more effective in the big-data collection, analysis, and intelligence reporting mission space, first-order analysis (initial characterization and correlation) must be a contact sport that is, must happen at the point and time of contact with the data on the sensor. This paper will use actionable examples: (1) to advocate for running Machine-Learning (ML) algorithms on the sensor as it will result in more timely, more accurate (fewer false positives), automated, scalable, and usable analyses; (2) discuss why establishing thought-diverse (variety of opinions, perspectives, and positions) analytic teams to perform and produce analysis will not only result in more effective collection, analysis, and ability to counter and/or neuter U.S. networks.
"
2015,Poletaeva2015,"The notion of “extrapolation ability” was developed by L.V. Krushinsky. His concept of animal reasoning, signifies the ability of animals to anticipate the position of (food) stimuli after their translocation and disappearance from the animals' view. Experiments proved this ability is not a simple trait, but requires a constellation of various optimal cognitive functions. Only several genetic groups among laboratory rodents are able to anticipate food reward on the basis of extrapolation, as opposed to instrumental learning. The paper includes data on extrapolation ability in mice with various chromosomal rearrangements, showing non-random performance on extrapolation tasks in animals carrying specific mutations. Experiments in which mice were selected for extrapolation ability demonstrate concomitant changes in other cognitive tasks and traits (fear-anxiety, reactions to novelty). Future studies should involve both the combination of several experimental paradigms and correlational analysis to further delineate the genetic underpinnings of anticipation as expressed in the extrapolation ability.
"
2015,Cois2015,"Addressing security in the software development lifecycle is an ever-present concern for software engineers and organizations. From a management and monitoring perspective, it is difficult to measure 1) the amount of effort being focused on security concerns during active development and 2) the success of security related design and development efforts. Such data is simply not recorded. If reliable measurements were available, software project leaders would have a powerful tool to assess risk and inform decision making. This would enable managers to direct development and testing to assure a desired level of security in their software products, to protect both their organizations and customers. To fill this need and provide such data, we propose a technique for performing topic detection on data commonly available in most software development projects: text artifacts from issue tracking and version control systems. We apply machine learning and natural language processing techniques to create classifiers capable of accurately detecting whether a given text snippet is related to the topic of security. Realization of such a capability will give software teams the ability to analyze current and past levels of security effort, revealing immediate project focus and the long-term impacts of security tasking. We validate our approach via experiments on data from the large-scale open source Chromium software project. Our results show that a Naïve Bayes classification scheme using an n-gram feature-space is an appropriate and effective approach to automated topic detection of software security text snippets, and that effective training data can be derived from public data sources without the need for manual intervention."
2017,Sapegin2017,"After almost two decades of development, modern Security Information and Event
Management (SIEM) systems still face issues with normalisation of heterogeneous data sources, high number of false positive alerts and long analysis times, especially in large-scale networks with high volumes of security events. In this paper, we present our own prototype of SIEM system, which is capable of dealing with these issues. For efficient data processing, our system employs in-memory data storage (SAP HANA) and our own technologies from the previous work, such as the Object Log Format (OLF) and high-speed event normalisation. We analyse normalised data using a combination of three different
approaches for security analysis: misuse detection, query-based analytics, and anomaly detection. Compared to the previous work, we have significantly improved our unsupervised anomaly detection algorithms. Most importantly, we have developed a novel hybrid outlier
detection algorithm that returns ranked clusters of anomalies. It lets an operator of a SIEM system to concentrate on the several top-ranked anomalies, instead of digging through an unsorted bundle of suspicious events. We propose to use anomaly detection in a combination with signatures and queries, applied on the same data, rather than as a full replacement for misuse detection. In this case, the majority of attacks will be captured with misuse detection, whereas anomaly detection will highlight previously unknown behaviour or attacks. We also propose that only the most suspicious event clusters need to be checked by an operator, whereas other anomalies, including false positive alerts, do not need to be explicitly checked if they have a lower ranking. We have proved our concepts and algorithms on a dataset of 160 million events from a network segment of a big multinational company and suggest that our approach and methods are highly relevant for modern SIEM systems."
2011,Francois2011,"With large scale botnets emerging as one of the major current threats, the automatic detection of botnet traffic is of high importance for service providers and large campus network monitoring. Faced with high speed network connections, detecting botnets must be efficient and accurate. This paper proposes a novel approach for this task, where NetFlow related data is correlated and a host dependency model is leveraged for advanced data mining purposes. We extend the popular linkage analysis algorithm PageRank [27] with an additional clustering process in order to efficiently detect stealthy botnets using peer-to-peer communication infrastructures and not exhibiting large volumes of traffic. The key conceptual component in our approach is to analyze communication behavioral patterns and to infer potential botnet activities.
"
2017,Mao2017,"System objects play different roles in computer systems and exhibit different levels of importance to system security. Assessing the importance of system objects helps us develop effective security protection methods. However, little work has focused on understanding and assessing the importance of system objects from a security perspective. In this paper, we build a security dependency network from access behaviors to quantify the security importance of system objects from a system-wide perspective. Similar to other networked systems, we observe small-world effect and power-law distributions for in and out-degree in the security dependency network. Exploring rich network structures in the security dependency network provides insights into the importance of system objects in security. We assess the importance of system objects, with respect to security, by the centrality metrics and propose an importance based model for malware detection. We evaluate importance metrics of system objects from various perspectives to demonstrate their feasibility and practicality. Furthermore, extensive experimental results on a real-world dataset demonstrate that our model is capable of detecting 7257 malware samples from 27,840 benign processes with a 93.92% true positive rate at 0.1% false positive rate.
"
2014,Garcia2014,"The results of botnet detection methods are usually presented without any comparison. Although it is generally accepted that more comparisons with third-party methods may help to improve the area, few papers could do it. Among the factors that prevent a comparison are the difficulties to share a dataset, the lack of a good dataset, the absence of a proper description of the methods and the lack of a comparison methodology. This paper compares the output of three different botnet detection methods by executing them over a new, real, labeled and large botnet dataset. This dataset includes botnet, normal and background traffic. The results of our two methods (BClus and CAMNEP) and BotHunter were compared using a methodology and a novel error metric designed for botnet detections methods. We conclude that comparing methods indeed helps to better estimate how good the methods are, to improve the algorithms, to build better datasets and to build a comparison methodology.
"
2014,Paul2014,"As software-intensive systems become more and more complex, so does the assessment of the risks that these systems may have on people's businesses, privacy, livelihoods, and very lives. For very large long-lived industrial programmes, such as the Galileo programme of the European Space Agency (ESA), or the French Pentagon programme for the Ministry of Defence, traditional risk management approaches are now reaching their limit. This is true for tooling, but even more so for humans. This paper proposes novel techniques to deal with cognitive scalability issues in risk assessment studies, amongst which graphical extensions to traditional risk management approaches, such as chain diagrams, and the seamless integration of attack trees. Feedback and results were collected from security experts and other stakeholders, in a large industrial context (namely, the Galileo risk assessment programme) and through dedicated research and development demonstrations. The feedback and results show effective improvements with respect to standard practices, even though fine tuning is still needed to reach an adequate and financially acceptable equilibrium between: (i) dealing with a large number of small independent problems; and (ii) maintaining an overall understanding of the system's risks and risks treatment.
"
2015,Tosh2015a,"The initiative to protect against future cyber crimes requires a collaborative effort from all types of agencies spanning industry, academia, federal institutions, and military agencies. Therefore, a Cybersecurity Information Exchange (CYBEX) framework is required to facilitate breach/patch related information sharing among the participants (firms) to combat cyber attacks. In this paper, we formulate a non-cooperative cybersecurity information sharing game that can guide: (i) the firms (players) to independently decide whether to “participate in CYBEX and share” or not; (ii) the CYBEX framework to utilize the participation cost dynamically as incentive (to attract firms toward self-enforced sharing) and as a charge (to increase revenue). We analyze the game from an evolutionary gametheoretic strategy and determine the conditions under which the players' self-enforced evolutionary stability can be achieved. We present a distributed learning heuristic to attain the evolutionary stable strategy (ESS) under various conditions. We also show how CYBEX can wisely vary its pricing for participation to increase sharing as well as its own revenue, eventually evolving toward a win-win situation.
"
2016,Feng2016,"-Recently there has been increased attention to the consequences of architecture design decisions and their impact on security. Architectural design decisions have been identified as being critical for achieving high levels of software system security. However the majority of this research has been anecdotal and there are few tools or methods for understanding the architectural relations among files, and their impact on security. In this paper we employ a DRSpace-based analysis approach to identify architectural design flaws and we show, via an empirical study of 10 open source projects, that areas of a software architecture that suffer from greater numbers of design flaws are highly correlated with security bugs, and high levels of churn associated with those security bugs. Finally, we show that a specific type of design flaw-unstable interface-is correlated with the greatest increase in software security bugs.
"
2017,Zeem2017,"Identity theft, fraud, and abuse are problems affecting the entire society. Identity theft is often a “gateway” crime, as criminals use stolen or fraudulent identities to steal money, claim eligibility for services, hack into networks without authorization, and so on. The available data describing identity crimes and their aftermath are often in the form of recorded stories and reports by the news press, fraud examiners, and law enforcement. All of these sources are unstructured. In order to analyze identity theft data, this research proposes an approach which involves the novel collection of online news stories and reports on the topic of identity theft. Our approach pre-processes the raw text and extracts semi-structured information automatically, using text mining techniques. This paper presents statistical analysis of behavioral patterns and resources used by thieves and fraudsters to commit identity theft, including the identity attributes commonly linked to identity crimes, resources thieves employ to conduct identity crimes, and temporal patterns of criminal behavior. Furthermore, the automatically extracted information is validated against manually investigated news stories. Analyses of these results increase empirical understanding of identity threat behaviors, offer early warning signs of identity theft, and thwart future identity theft crimes."
2013,Clark2013,"Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this “hierarchical prediction machine” approach, concluding that it offers the best clue yet to the shape of a unified science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency.
"
2016,ChenKazman2016,"The number and variety of cyber-attacks is rapidly increasing, and the rate of new software vulnerabilities is also rising dramatically. The cybersecurity community typically reacts to attacks after they occur. Being reactive is costly and can be fatal, where attacks threaten lives, important data, or mission success. Taking a proactive approach, we are: (I) identifying potential attacks before they come to fruition, and based on this identification; (II) developing preventive counter-measures. We describe a Proactive Cybersecurity System (PCS), a layered, modular service platform that applies big data collection and processing tools a wide variety of unstructured data sources to identify potential attacks and develop countermeasures. The PCS provides security analysts a holistic, proactive, and systematic approach to cybersecurity. Here we describe our research vision and progress towards that vision.
"
2011,Markey2011,"As cyber threats grow in sophistication, network defenders need to use every tool in
the defensive arsenal to protect their networks. This paper presents a repeatable process to implement the decision tree technique on a small set of network data. Using this process, a security team can gather data, build a decision tree model, and incorporate the model’s logic into Snort signatures, firewall rules, and custom-built detection scripts. The process presented in this paper can serve as a preliminary test to determine the value of data mining techniques before deciding whether or not to incorporate the techniques across the enterprise. Alternatively, the proposed methodology can be used to implement ad-hoc decision tree analysis as the security data is available. Either approach allows corporations or security teams to quickly, easily, and inexpensively implement decision tree analysis and gain unique security insights based on the corporation's network data."
2012,Chen2012,"Researchers have proposed various metrics based on measurable aspects of the source code entities (e.g., methods, classes, files, or modules) and the social structure of a software project in an effort to explain the relationships between software development and software defects. However, these metrics largely ignore the actual functionality, i.e., the conceptual concerns, of a software system, which are the main technical concepts that reflect the business logic or domain of the system. For instance, while lines of code may be a good general measure for defects, a large entity responsible for simple I/O tasks is likely to have fewer defects than a small entity responsible for complicated compiler implementation details. In this paper, we study the effect of conceptual concerns on code quality. We use a statistical topic modeling technique to approximate software concerns as topics; we then propose various metrics on these topics to help explain the defect-proneness (i.e., quality) of the entities. Paramount to our proposed metrics is that they take into account the defect history of each topic. Case studies on multiple versions of Mozilla Firefox, Eclipse, and Mylyn show that (i) some topics are much more defect-prone than others, (ii) defect-prone topics tend to remain so over time, and (iii) defect-prone topics provide additional explanatory power for code quality over existing structural and historical metrics.
"
2012,Bex2012,"In this paper, we look at reasoning with evidence and facts in criminal cases. We show how this reasoning may be analysed in a dialectical way by means of critical questions that point to typical sources of doubt. We discuss critical questions about the evidential arguments adduced, about the narrative accounts of the facts considered, and about the way in which the arguments and narratives are connected in an analysis. Our treatment shows how two different types of knowledge, represented as schemes, play a role in reasoning with evidence: argumentation schemes and story schemes.
"
2014,Walden2014,"Building secure software is difficult, time consuming, and expensive. Prediction models that identify vulnerability prone software components can be used to focus security efforts, thus helping to reduce the time and effort required to secure software. Several kinds of vulnerability prediction models have been proposed over the course of the past decade. However, these models were evaluated with differing methodologies and datasets, making it difficult to determine the relative strengths and weaknesses of different modeling techniques.
"
2017,Lagerstrom2017,"Employing software metrics, such as size and
complexity, for predicting defects has been given a lot of
attention over the years and proven very useful. However, the
few studies looking at software architecture and vulnerabilities
are limited in scope and findings. We explore the relationship
between software vulnerabilities and component metrics (like
code churn and cyclomatic complexity), as well as architecture
coupling metrics (direct, indirect, and cyclic coupling). Our case
is based on the Google Chromium project, an open source
project that has not been studied for this topic yet. Our findings
show a strong relationship between vulnerabilities and both
component level metrics and architecture coupling metrics.
Unfortunately, the effects of different types of coupling are
somewhat hard to distinguish."
2017,Chen2017,"The cybersecurity community typically reacts to attacks after they occur. Being reactive is costly and can be fatal where attacks threaten lives, important data, or mission success. But can cybersecurity be done proactively? Our research capitalizes on the Germination Period-the time lag between hacker communities discussing software flaw types and flaws actually being exploited-where proactive measures can be taken. We argue for a novel proactive approach, utilizing big data, for (I) identifying potential attacks before they come to fruition; and based on this identification, (II) developing preventive countermeasures. The big data approach resulted in our vision of the Proactive Cybersecurity System (PCS), a layered, modular service platform that applies big data collection and processing tools to a wide variety of unstructured data sources to predict vulnerabilities and develop countermeasures. Our exploratory study is the first to show the promise of this novel proactive approach and illuminates challenges that need to be addressed.
"
2015,BenAsher2015b,"Human cognitive and analytical capabilities are needed and are indispensable to success in cyber defense. However, the high volume of network data challenges the process of detecting cyber-attacks, especially zero-day attacks. Training along with detailed and timely outcome feedback is a major factor in improving performance. It supports attributes identification and rule formation, which are crucial to the detection of attacks. To understand the role of feedback during training and how it influences the detection of novel attacks, we developed a simplified Intrusion Detection System and trained 160 participants to perform as analysts. During training, participants classified network events representing a specific cyber-attack, and received feedback at the end of each trial. Detailed feedback used color schemes informing of hits, misses, false-alarms, and correct-rejections. Aggregated feedback provided numerical summaries regarding performance. After training, participants classified events that were similar or part of a novel attack. Results show that detailed feedback accelerated learning and improved detection accuracy compared to aggregated feedback. Participants who received aggregated feedback failed to learn the role of certain network attributes and how to integrate them into detection rules. Surprisingly, aggregated feedback improved detection in the novel attack. The novelty of a situation caused an increase in decision scrutiny, while familiar decision situations limited the depth of information search and evaluation. Analyst should learn to abstract information and look broadly at outcome feedback to improve their ability to detect novel attacks. We discuss the implications of these findings for improving cyber security.
"
2013,Schlamp2013,"The Border Gateway Protocol (BGP) was designed without security in mind. Until today, this fact makes the Internet vulnerable to hijacking attacks that intercept or blackhole Internet traffic. So far, significant effort has been put into the detection of IP prefix hijacking, while AS hijacking has received little attention. AS hijacking is more sophisticated than IP prefix hijacking, and is aimed at a long-term benefit such as over a duration of months. In this paper, we study a malicious case of AS hijacking, carried out in order to send spam from the victim's network. We thoroughly investigate this AS hijacking incident using live data from both the control and the data plane. Our analysis yields insights into how an attacker proceeded in order to covertly hijack a whole autonomous system, how he misled an upstream provider, and how he used an unallocated address space. We further show that state of the art techniques to prevent hijacking are not fully capable of dealing with this kind of attack. We also derive guidelines on how to conduct future forensic studies of AS hijacking. Our findings show that there is a need for preventive measures that would allow to anticipate AS hijacking and we outline the design of an early warning system.
"
2014,Noel2014,"We describe a suite of metrics for measuring network-wide cyber security risk based on a model of multi-step attack vulnerability (attack graphs). Our metrics are grouped into families, with family-level metrics combined into an overall metric for network vulnerability risk. The Victimization family measures risk in terms of key attributes of risk across all known network vulnerabilities. The Size family is an indication of the relative size of the attack graph. The Containment family measures risk in terms of minimizing vulnerability exposure across protection boundaries. The Topology family measures risk through graph theoretic properties (connectivity, cycles, and depth) of the attack graph. We display these metrics (at the individual, family, and overall levels) in interactive visualizations, showing multiple metrics trends over time.
"
2015,Deka2015,"To defend a network from intrusion is a generic problem of all time. It is important to develop a defense
mechanism to secure the network from anomalous activities. This paper presents a comprehensive
survey of methods and systems introduced by researchers in the past two decades to protect network
resources from intrusion. A detailed pros and cons analysis of these methods and systems is also
reported in this paper. Further, this paper also provides a list of issues and research challenges in this
evolving field of research. We believe that this knowledge will help to create a defense system."
2012,Ding2012,"A reasonable definition of intrusion is: entering a community to which one does not belong. This suggests that in a network, intrusion attempts may be detected by looking for communication that does not respect community boundaries. In this paper, we examine the utility of this concept for identifying malicious network sources. In particular, our goal is to explore whether this concept allows a core-network operator using flow data to augment signature-based systems located at network edges. We show that simple measures of communities can be defined for flow data that allow a remarkably effective level of intrusion detection simply by looking for flows that do not respect those communities. We validate our approach using labeled intrusion attempt data collected at a large number of edge networks. Our results suggest that community-based methods can offer an important additional dimension for intrusion detection systems.
"
2017,DangPham2017b,"The rapid digital transformation and technological disruption in modern organisations demand the development of people-centric security workplaces, whereby the employees can build up their security awareness and accountability for their actions via participation in the organisation's social networks. The social network analysis approach offers a wide array of analytical capabilities to examine in-depth the interactions and relations within an organisation, which assists the development of such security workplaces. This paper proposes the novel and practical adoption of social network analysis methods in behavioural information security field. To this end, we discuss the core features of the social network analysis approach and describe their empirical applications in a real case study of a large organisation in Vietnam, which utilised these methods to improve employees' information security awareness. Towards the end of the paper, a framework detailing the strategies for conducting social network analysis in the behavioural information security field is developed and presented.
"
2013,Bilar2013,"It is well known that computer and network security is an adversarial challenge. Attackers develop exploits and defenders respond to them through updates, service packs or other defensive measures. In non-adversarial situations, such as automobile safety, advances on one side are not countered by the other side and so progress can be demonstrated over time. In adversarial situations, advances by one side are countered by the other and so oscillatory performance typically emerges. This paper contains a detailed study of the coevolution of the Conficker Worm and associated defenses against it. It demonstrates, in concrete terms, that attackers and defenders each present moving targets to the other. After detailing specific adaptations of attackers and defenders in the context of Conficker and its variants, we briefly develop a quantitative model for explaining the coevolution based on what we call Quantitative Attack Graphs (QAG) which involve attackers selecting shortest paths through an attack graph with defenders investing in hardening the shortest path edges appropriately.
"
2010,Sommer2010,"-In network intrusion detection research, one popular strategy for finding attacks is monitoring a network's activity for anomalies: deviations from profiles of normality previously learned from benign traffic, typically identified using tools borrowed from the machine learning community. However, despite extensive academic research one finds a striking gap in terms of actual deployments of such systems: compared with other intrusion detection approaches, machine learning is rarely employed in operational “real world” settings. We examine the differences between the network intrusion detection problem and other areas where machine learning regularly finds much more success. Our main claim is that the task of finding attacks is fundamentally different from these other applications, making it significantly harder for the intrusion detection community to employ machine learning effectively. We support this claim by identifying challenges particular to network intrusion detection, and provide a set of guidelines meant to strengthen future research on anomaly detection.
"
2017,Moriano2017,"Routing anomalies, beyond simple leaks, are occurring on the order of tens of thousands a
year. These may be accidents, but there is anecdotal evidence that indicates criminal intent.
There are case studies that illustrate the use of these for national intelligence. Any given anomaly
could be an accident, a crime, or an attack. Although it is impossible to directly observe the
motivation of those who generate these anomalies, aggregate data about the sources of these
anomalies is available. Here we leverage tools of macroeconomics to provide insights into the
possible nature of these anomalies. We offer an empirical investigation using multiple linear
regression and unsupervised learning to analyze data over a four-year period in order to better
understand the nature of routing anomalies. If routing anomalies are a result of limited
technical competence, then countries with low levels of education, few technology exports, and
less expertise should be over-represented. If routing anomalies are leveraged by criminals for
profit, then economic theories and analytical approaches from criminology should show
statistical significance. Or, if routing anomalies are primarily used by national intelligence agencies
to attack either internal dissidents or those outside their borders, then the presence of
conflict and measures of quality of governance are possible indicators. We examine anomalies
as likely due to incompetence, potential ecrime, or intelligence operations using
macroeconomics by leveraging three theories from criminology and global measures of technology
adoption. We found that exports of technology were not statistically significant,
undermining the argument for incompetence. We also found support for the possibility that anomalies
are driven by crime, specifically for the guardianship and relative deprivation theories of crime.
In addition to these findings from regression analysis, clustering indicates that civil conflict
and surveillance are associated with the disproportionate origination of routing anomalies.
This supports the possibility of use of routing anomalies for national intelligence."
2017,Esteves2017,"Cyberattacks are an increasingly common and worrisome threat. To combat the risk, companies need to understand both hackers’ tactics and their mindsets."
2006,Yang2006,"Prediction in streaming data is an important activity in the modern society. Two major challenges posed by data streams are (1) the data may grow without limit so that it is difficult to retain a long history of raw data; and (2) the underlying concept of the data may change over time. The novelties of this paper are in four folds. First, it uses a measure of conceptual equivalence to organize the data history into a history of concepts. This contrasts to the common practice that only keeps recent raw data. The concept history is compact while still retains essential information for learning. Second, it learns concept-transition patterns from the concept history and anticipates what the concept will be in the case of a concept change. It then proactively prepares a prediction model for the future change. This contrasts to the conventional methodology that passively waits until the change happens. Third, it incorporates proactive and reactive predictions. If the anticipation turns out to be correct, a proper prediction model can be launched instantly upon the concept change. If not, it promptly resorts to a reactive mode: adapting a prediction model to the new data. Finally, an efficient and effective system RePro is proposed to implement these new ideas. It carries out prediction at two levels, a general level of predicting each oncoming concept and a specific level of predicting each instance's class. Experiments are conducted to compare RePro with representative existing prediction methods on various benchmark data sets that represent diversified scenarios of concept change. Empirical evidence offers inspiring insights and demonstrates the proposed methodology is an advisable solution to prediction in data streams.
"
2015,Friedberg2015,"With a major change in the attack landscape, away from well-known attack vectors towards unique and highly tailored attacks, limitations of common rule- and signature-based security systems become more and more obvious. Novel security mechanisms can provide the means to extend existing solutions in order to provide a more sophisticated security approach. As critical infrastructures get increasingly accessible from public networks they show up on attackers' radars. As a consequence, establishing cyber situational awareness on a higher level through incident information sharing is vital for assessing the increased risk to national security in the cyber space. But legal obligations and economical considerations limit the motivation of companies to pursue information sharing initiatives. To support companies and governmental initiatives, novel security mechanisms should inherently address limiting factors. One novel approach, AECID, is presented that accounts for the limitations of many common intrusion and anomaly detection mechanisms; and which further provides the features to support privacy-aware information sharing for cyber situational awareness.
"
2015,Mo2015,"-In this paper, we propose and empirically validate a suite of hotspot patterns: recurring architecture problems that occur in most complex systems and incur high maintenance costs. In particular, we introduce two novel hotspot patterns, Unstable Interface and Implicit Cross-module Dependency. These patterns are defined based on Baldwin and Clark's design rule theory, and detected by the combination of history and architecture information. Through our tool-supported evaluations, we show that these patterns not only identify the most error-prone and change-prone files, they also pinpoint specific architecture problems that may be the root causes of bug-proneness and change-proneness. Significantly, we show that 1) these structurehistory integrated patterns contribute more to error- and changeproneness than other hotspot patterns, and 2) the more hotspot patterns a file is involved in, the more error- and change-prone it is. Finally, we report on an industrial case study to demonstrate the practicality of these hotspot patterns. The architect and developers confirmed that our hotspot detector discovered the majority of the architecture problems causing maintenance pain, and they have started to improve the system's maintainability by refactoring and fixing the identified architecture issues.
"
2015,Edkrantz2015,"Every day numerous new vulnerabilities and exploits are reported for a wide variety of different software configurations. There is a big need to be able to quickly assess associated risks and sort out which vulnerabilities that are likely to be exploited in real-world attacks. A small percentage of all vulnerabilities account for almost all the observed attack volume. We use machine learning to make automatic predictions for unseen vulnerabilities based on previous exploit patterns. Index Terms-Machine Learning, Information Security, Vulnerability Prediction
"
2015,Perl2015,"Despite the security community's best effort, the number of serious vulnerabilities discovered in software is increasing rapidly. In theory, security audits should find and remove the vulnerabilities before the code ever gets deployed. However, due to the enormous amount of code being produced, as well as a the lack of manpower and expertise, not all code is su efficiently audited. Thus, many vulnerabilities slip into production systems. A best-practice approach is to use a code metric analysis tool, such as Flaw Finder, to flag potentially dangerous code so that it can receive special attention. However, because these tools have a very high false-positive rate, the manual effort needed to find vulnerabilities remains overwhelming. In this paper, we present a new method of finding potentially dangerous code in code repositories with a significantly lower false-positive rate than comparable systems. We combine code-metric analysis with metadata gathered from code repositories to help code review teams prioritize their work. The paper makes three contributions. First, we conducted the first large-scale mapping of CVEs to GitHub commits in order to create a vulnerable commit database. Second, based on this database, we trained a SVM classifier to flag suspicious commits. Compared to Flaw Finder, our approach reduces the amount of false alarms by over 99 % at the same level of recall. Finally, we present a thorough quantitative and qualitative analysis of our approach and discuss lessons learned from the results. We will share the database as a benchmark for future research and will also provide our analysis tool as a web service.
"
2015,Abunadi2015,"Building secure software is challenging, time-consuming, and expensive. Software vulnerability prediction models that identify vulnerable software components are usually used to focus security efforts, with the aim of helping to reduce the time and effort needed to secure software. Existing vulnerability prediction models use process or product metrics and machine learning techniques to identify vulnerable software components. Cross project vulnerability prediction plays a significant role in appraising the most likely vulnerable software components, specifically for new or inactive projects. Little effort has been spent to deliver clear guidelines on how to choose the training data for project vulnerability prediction. In this work, we present an empirical study aiming at clarifying how useful cross project prediction techniques in predicting software vulnerabilities. Our study employs the classification provided by different machine learning techniques to improve the detection of vulnerable components. We have elaborately compared the prediction performance of five well-known classifiers. The study is conducted on a publicly available dataset of several PHP open source web applications and in the context of cross project vulnerability prediction, which represents one of the main challenges in the vulnerability prediction field."
2010,Roy2010,"Network security is a complex and challenging problem. The area of network defense mechanism design is receiving immense attention from the research community for more than two decades. However, the network security problem is far from completely solved. Researchers have been exploring the applicability of game theoretic approaches to address the network security issues and some of these approaches look promising. This paper surveys the existing game theoretic solutions which are designed to enhance network security and presents a taxonomy for classifying the proposed solutions. This taxonomy should provide the reader with a better understanding of game theoretic solutions to a variety of cyber security problems.
"
2014,Sunday2014,"Anticipatory failure determination, abbreviated as AFD, is qualitative risk analysis approach that is based on I-TRIZ, a Russian short form of theory of inventive problem solving. Although this approach comprises two methods, AFD-1 and AFD-2, which are methodically structured to capture failure scenario that has occurred or predict and reveal future failures that have not occurred respectively, it has shortcomings and weaknesses which are very important to address and eliminate.
In this thesis, weaknesses and shortcomings of the approach are explored, AFD-1 and AFD-2 are modified, and new AFD method named AFD-3 is created. The two modified AFD methods and newly created AFD-3 are based on SIVAI-TRIZ which is extended form of I-TRIZ for solving inventive problem. The modified methods and newly created AFD-3 offer higher degree of flexibility, effectiveness, and empowerment to reveal, predict and capture system failure scenarios. SIVAI- TRIZ body of knowledge is realized by applying system design approach to risk analysis through embedding design structure matrix (DSM) and design matrix (DM) of axiomatic design on AFD methods to address AFD shortcomings and augment I-TRIZ body of knowledge."
2017,Nguyen2017,"Considering the gaining popularity of ”defense in 7 depth” strategy, plus increasing amounts of money invested in information security layers, and considering adversaries' perspective while carrying out a long-term advanced-persistent attack campaign; avoiding (short term) detections may not be as beneficial as having a deeper knowledge about targeted ”defense y in depth” system. Probing and stealing information security a machine learning models for organized cyber attack campaigns should not focus only on obvious results (a yes/no classification of attacks) but also on other factors.
"
2015,Thurnes2015,"The Failure Mode and Effects Analysis (FMEA) is a very highly established methodology for preventing failures in technical systems [1] that developed over the last five decades. It became a standard tool, especially since the introduction to the automotive industry via the QS-9000. Despite of numerous successful applications the FMEA is limited to normal expectations of occurring failures (like the non-fulfillment of a function or small deviation from an expected value) [2]. The FMEA utilizes the breakdown structure for products or processes to identify single failure causes and effects. Usually the FMEA provides no consideration of interconnected failures and failure scenarios. There are a lot of good reasons for the usage of the FMEA: it is wide spread because of being a part of international standards and engineering education for a long time. To cover these drawbacks described above, additional tools for risk analysis and risk management can be used. The Anticipatory Failure Prediction as preventive component of the Anticipatory Failure Determination (AFD) is such a method that leads to a comprehensive set of failures and failure scenarios [3], [4]. Additionally it provides procedures to “invent” possible failures in a structured but creative way [5]. Based on the analysis of both methods, ways to hybridize the methods are developed. This hybridization leads to the combination of the advantages of both methods (similar to the parallel execution of both methods), offers synergies, and expands the potential for industrial adoption by providing one elaborated tool. By using an application example the potential of the hybridized AFD-FMEA or so called Failure Mode and Effects Anticipation and Analysis (FMEAA) is pointed out.
"
2016,Buczak2016,"-This survey paper describes a focused literature survey of machine learning (ML) and data mining (DM) methods for cyber analytics in support of intrusion detection. Short tutorial descriptions of each ML/DM method are provided. Based on the number of citations or the relevance of an emerging method, papers representing each method were identified, read, and summarized. Because data are so important in ML/DM approaches, some well-known cyber data sets used in ML/DM are described. The complexity of ML/DM algorithms is addressed, discussion of challenges for using ML/DM for cyber security is presented, and some recommendations on when to use a given method are provided.
"
2003,Lindqvist2003,"ROME, NEW YORK
"
2010,Bubic2010,"The term “predictive brain” depicts one of the most relevant concepts in cognitive neuroscience which emphasizes the importance of “looking into the future”, namely prediction, preparation, anticipation, prospection or expectations in various cognitive domains. Analogously, it has been suggested that predictive processing represents one of the fundamental principles of neural computations and that errors of prediction may be crucial for driving neural and cognitive processes as well as behavior. This review discusses research areas which have recognized the importance of prediction and introduces the relevant terminology and leading theories in the field in an attempt to abstract some generative mechanisms of predictive processing. Furthermore, we discuss the process of testing the validity of postulated expectations by matching these to the realized events and compare the subsequent processing of events which confirm to those which violate the initial predictions. We conclude by suggesting that, although a lot is known about this type of processing, there are still many open issues which need to be resolved before a unified theory of predictive processing can be postulated with regard to both cognitive and neural functioning."
2009,Stroeh2009,"Organizations face the ever growing challenge of providing security within their IT infrastructures. Static approaches to security, such as perimetral defense, have proven less than effective - and, therefore, more vulnerable - in a new scenario characterized by increasingly complex systems and by the evolution and automation of cyber attacks. Moreover, dynamic detection of attacks through IDSs (Instrusion Detection Systems) presents too many false positives to be effective. This work presents an approach on how to collect and normalize, as well as how to fuse and classify, security alerts. This approach involves collecting alerts from different sources and normalizes them according to standardized structures - IDMEF (Intrusion Detection Message Exchange Format). The normalized alerts are grouped into meta-alerts (fusion, or clustering), which are later classified using machine learning techniques into attacks or false alarms. We validate and report an implementation of this approach against the DARPA Challenge and the Scan of the Month, using three different classifications - SVMs, Bayesian Networks and Decision Trees - having achieved high levels of attack detection with little false positives. Our results also indicate that our approach outperforms other works when it comes to detecting new kinds of attacks, making it more suitable to a world of evolving attacks.
"
2011,Rieck2011a,"Malicious software - so called malware - poses a major threat to the security of computer systems. The amount and diversity of its variants render classic security defenses ineffective, such that millions of hosts in the Internet are infected with malware in the form of computer viruses, Internet worms and Trojan horses. While obfuscation and polymorphism employed by malware largely impede detection at file level, the dynamic analysis of malware binaries during run-time provides an instrument for characterizing and defending against the threat of malicious software. In this article, we propose a framework for the automatic analysis of malware behavior using machine learning. The framework allows for automatically identifying novel classes of malware with similar behavior (clustering) and assigning unknown malware to these discovered classes (classification). Based on both, clustering and classification, we propose an incremental approach for behavior-based analysis, capable of processing the behavior of thousands of malware binaries on a daily basis. The incremental analysis significantly reduces the run-time overhead of current analysis methods, while providing accurate discovery and discrimination of novel malware variants.
"
2017,Musman2017,"This paper describes the Cyber Security Game (CSG). Cyber Security Game is a method that has been implemented in software that quantitatively identifies cyber security risks and uses this metric to determine the optimal employment of security methods for any given investment level. Cyber Security Game maximizes a system's ability to operate in today's contested cyber environment by minimizing its mission risk. The risk score is calculated by using a mission impact model to compute the consequences of cyber incidents and combining that with the likelihood that attacks will succeed. The likelihood of attacks succeeding is computed by applying a threat model to a system topology model and defender model. Cyber Security Game takes into account the widespread interconnectedness of cyber systems, where defenders must defend all multi-step attack paths and an attacker only needs one to succeed. It employs a game theoretic solution using a game formulation that identifies defense strategies to minimize the maximum cyber risk (MiniMax). This paper discusses the methods and models that compose Cyber Security Game . A limited example of a Point of Sale system is used to provide specific demonstrations of Cyber Security Game models and analyses.
"
2017,Cao2017,"WHILE DATA SCIENCE has emerged as an ambitious new scientific field, related debates and discussions have sought to address why science in general needs data science and what even makes data science a science. However, few such discussions concern the intrinsic complexities and intelligence in data science problems and the gaps in and opportunities for data science research. Following a comprehensive literature review, I offer a number of observations concerning big data and the data science debate. For example, discussion has covered not only data-related disciplines and domains like statistics, computing, and informatics but traditionally less data-related fields and areas like social science and business management as well. Data science has thus emerged as a new inter- and cross-disciplinary field. Although many publications are available, most (likely over 95%) concern existing concepts and topics in statistics, data mining, machine learning, and broad data analytics. This limited view demonstrates how data science has emerged from existing core disciplines, particularly statistics, computing, and informatics. The abuse, misuse, and overuse of the term “data science” is ubiquitous, contributing to the hype, and myths and pitfalls are common. While specific challenges have been covered, few scholars have addressed the low-level complexities and problematic nature of data science or contributed deep insight about the intrinsic challenges, directions, and opportunities of data science as an emerging field."
2011,Albanese2011,"Attack graphs have been widely used for attack modeling, alert correlation, and prediction. In order to address the limitations of current approaches - scalability and impact analysis - we propose a novel framework to analyze massive amounts of alerts in real time, and measure the impact of current and future attacks. Our contribution is threefold. First, we introduce the notion of generalized dependency graph, which captures how network components depend on each other, and how the services offered by an enterprise depend on the underlying infrastructure. Second, we extend the classical definition of attack graph with the notion of timespan distribution, which encodes probabilistic knowledge of the attacker's behavior. Finally, we introduce attack scenario graphs, which combine dependency and attack graphs, bridging the gap between known vulnerabilities and the services that could be ultimately affected by the corresponding exploits. We propose efficient algorithms for both detection and prediction, and show that they scale well for large graphs and large volumes of alerts. We show that, in practice, our approach can provide security analysts with actionable intelligence about the current cyber situation, enabling them to make more informed decisions.
"
2015,Ahn2015,"Modern nuclear power plants (NPPs) use a variety of digital technologies, with new technologies such as wireless sensor networks also under active consideration. Consequently, like other critical infrastructure at increasing risk of cyber-attacks, cyber security for NPPs has become an issue of growing concern. The development of cyber-attack scenarios is therefore a crucial part of cyber security plans for NPPs. However, regulatory guidelines for NPPs only mention the necessity for cyber-attack scenarios, their application methods, and simple examples, without outlining any systematic method to actually develop comprehensive cyber-attack scenarios. This paper proposes a method for developing NPP cyber-attack scenarios that reflect the inherent characteristics and architecture of NPPs using scenario graphs, a type of attack model. Further, the process of modeling scenario graphs using the proposed method and a process for developing new attack scenarios by combining several scenario graphs are discussed. In the scenario graphs, large and complicated system structures are conceptually divided, facilitating identification of components of cyber-attacks. Attack scenarios developed using these scenario graphs can be used extensively to develop design basis threats (DBTs), risk assessments, and penetration tests. Thus, they are important for establishing cyber security plans and programs.
"
2016,Bailetti2016b,"“ The illiterate of the 21st Century are not those ” who cannot read and write but those who cannot learn, unlearn and relearn. The purpose of this article is to provide a definition of intrusion learning, identify its distinctive aspects, and provide recommendations for advancing intrusion learning as a practice domain. The authors define intrusion learning as the collection of online network algorithms that learn from and monitor streaming network data resulting in effective intrusion-detection methods for enabling the security and resiliency of enterprise systems. The network algorithms build on advances in cyber-defensive and cyber-offensive capabilities. Intrusion learning is an emerging domain that draws from machine learning, intrusion detection, and streaming network data. Intrusion learning offers to significantly enhance enterprise security and resiliency through augmented perimeter defense and may mitigate increasing threats facing enterprise perimeter protection. The article will be of interest to researchers, sponsors, and entrepreneurs interested in enhancing enterprise security and resiliency.
"
2010,Bozorgi2010,"The security demands on modern system administration are enormous and getting worse. Chief among these demands, administrators must monitor the continual ongoing disclosure of software vulnerabilities that have the potential to compromise their systems in some way. Such vulnerabilities include buffer overflow errors, improperly validated inputs, and other unanticipated attack modalities. In 2008, over 7,400 new vulnerabilities were disclosed — well over 100 per week. While no enterprise is affected by all of these disclosures, administrators commonly face many outstanding vulnerabilities across the software systems they manage. Vulnerabilities can be addressed by patches, reconfigurations, and other workarounds; however, these actions may incur down-time or un- foreseen side-effects. Thus, a key question for systems administrators is which vulnerabilities to prioritize. From publicly available databases that document past vulnerabilities, we show how to train classifiers that predict whether and how soon a vulnerability is likely to be exploited. As input, our classifiers operate on high dimensional feature vectors that we extract from the text fields, time stamps, cross-references, and other entries in existing vulnerability disclosure reports. Compared to current industry-standard heuristics based on expert knowledge and static formulas, our classifiers predict much more accurately whether and how soon individual vulnerabilities are likely to be exploited."
2017,Hamman2017,"-The ability to anticipate the strategic actions of hackers, including where, when, and how they might attack, and their tactics for evading detection, is a valuable skill for cybersecurity. Therefore, developing the strategic reasoning abilities of cybersecurity students is an important cybersecurity education learning objective. This paper proposes that basic game theory concepts should be taught to cybersecurity students in order to improve their strategic reasoning abilities. It details a pretest-posttest educational experiment that demonstrates that 2 h of basic game theory instruction results in a statistically significant improvement in students' abilities to anticipate the strategic actions of others. It also provides details of the game theory curriculum to help other cybersecurity educators replicate these results. Additionally, this paper suggests that another benefit of teaching game theory in a cybersecurity course is that it may fundamentally alter the way students view the practice of cybersecurity, helping to sensitize them to the human adversary element inherent in cybersecurity in addition to technologyfocused best practices. This could result in a more naturally strategic-minded, and therefore better equipped, cybersecurity workforce.
"
2015,Frigotti2015,"Classical models of decision making deal fairly well with uncertainty, where settings are well-structured in terms of goals, alternatives, and consequences. Conversely, the typical ill-structured nature of strategy choices remains a challenge for extant models. Such cases can hardly build on the past, and their novelty makes the prediction of consequences a very difficult and poorly robust task. The weakness of the classical expected utility model in representing such problems has not been adequately solved by recent extensions. In this paper we offer an explanatory coherence model for decision making in ill-structured problems. We model alternatives as sets of concurrent causal explanations of reality that act as justifications for action. According to these premises, choice is based on an evaluation of the internal coherence and the consistency of competing explanations of the available evidence. This model is psychologically grounded on causal inference and builds on the connectionist tradition of explanatory coherence. To illustrate the model, we consider the decision of investing in a new technology and we discuss how changes in the structure of alternatives may impact on the solution. We show how the final choice depends on collecting the relevant evidence, making the suitable hypotheses, and drawing the consistent causal explanations linking the two.
"
2016,Huang2016,"Understanding the nature of a country’s World Wide Web security can allow analysts to evaluate the security awareness of local organizations, the technology employed by researchers, and the defense capabilities of the whole country. In this paper, we put forward a new framework to evaluate the security situation in China with real vulnerability disclosure platforms. The focus of this research is to analyze the current situation of Chinese websites using 57,112 Web vulnerability incidents submitted by 5371 researchers from 2012 to 2015. The dataset is distributed into four types of organizations, including listed companies, government institutions, educational institutions, and startups. We present an approach, based on machine learning and natural language processing technologies, to classify the vulnerability type for each incident. Furthermore, our experimental results show that the vulnerability distribution and response speed toward important issues are so different among the four types of organizations that researchers at various levels of experience begin to take part in submitting vulnerabilities to public disclosure platforms. Based on the results, we propose security some best-practices for organizations and show that the security situation of Chinese web-sites has changed quickly in the last three years but is still facing several big problems."
2016,Mezzour2016,"Online cyber threat descriptions are rich, but little research has attempted to systematically analyze these descriptions. In this paper, we process and analyze two of Symantec's online threat description corpora. The AntiVirus (AV) corpus contains descriptions of more than 12,400 threats detected by Symantec's AV, and the Intrusion Prevention System (IPS) corpus contains descriptions of more than 2,700 attacks detected by Symantec's IPS. In our analysis, we quantify the over time evolution of threat severity and type in the corpora. We also assess the amount of time Symantec takes to release signatures for newly discovered threats. Our analysis indicates that a very small minority of threats in the AV corpus are high-severity, whereas the majority of attacks in the IPS corpus are high-severity. Moreover, we find that the prevalence of different threat types such as worms and viruses in the corpora varies considerably over time. Finally, we find that Symantec prioritizes releasing signatures for fast propagating threats.
"
2014,Alpcan2014,"Game theory provides a mature mathematical foundation for making security decisions in a principled manner. Security games help formalizing security problems and decisions using quantitative models. The resulting analytical frameworks lead to better allocation of limited resources and result in more informed responses to security problems in complex systems and organizations. The game-theoretic approach to security is applicable to a wide variety of systems and critical infrastructures such as electricity, water, financial services, and communication networks."
2017,Bones2017b,"Only amateurs attack machines; professionals target people. Just as military battles are no longer fought in trenches, with massive armored vehicles clashing in open efilds, cyberwars are transform ing the battlespace of the future.* In military parlance, the introduction of urban fighters and mobile targets changed how proxy wars are fought. Asymmetric in execution, military leaders had to adapt to unconventional tactics in response to new threats. Cyber risk is also three dimensional in a digital sense. The first dimension is advanced technology, followed by cognitive hacks, with the end result being real collateral damage in time, expense, and reputation. In Chapter 1, we discussed the inherent weakness in building “Maginot Lines” to defend the fort with layers upon layers of security protocols that have proven ineefctive in preventing attack. The ques tion remains: If not some form of Maginot Lines, what has proven more eefctive? I explore that question by summarizing research find ings and asking more questions that remain unanswered. However, as the costs to defend and mitigate attacks escalate, senior management will demand ways to slow or lower the cost of cybersecurity. How will security professionals respond? What new approaches are available to improve security and lower the cost of defending the fortress? Firms must consider new ways to address the asymmetric nature of cyberattacks using their own toolkit of asymmetric defenses. One such set of new tools being explored by the military, government agencies, and a host of industries is the domain of human behavior and cognitive sciences. * http://news.usni.org/2012/10/14/asymmetric-nature-cyber-warfare
"
2004,SheynerWing2004,"Attack graphs depict ways in which an adversary exploits system vulnerabilities to achieve a desired state. System administrators use attack graphs to determine how vulnerable their systems are and to determine what security measures to deploy to defend their systems. In this paper, we present details of an example to illustrate how we specify and analyze network attack models. We take these models as input to our attack graph tools to generate attack graphs automatically and to analyze system vulnerabilities. While we have published our generation and analysis algorithms in earlier work, the presentation of our example and toolkit is novel to this paper.
"
2015,Heartfield2015,"Social engineering is used as an umbrella term for a broad spectrum of computer exploitations that employ a variety of attack vectors and strategies to psychologically manipulate a user. Semantic attacks are the specific type of social engineering attacks that bypass technical defences by actively manipulating object characteristics, such as platform or system applications, to deceive rather than directly attack the user. Commonly observed examples include obfuscated URLs, phishing emails, drive-by downloads, spoofed websites and scareware to name a few. This paper presents a taxonomy of semantic attacks, as well as a survey of applicable defences. By contrasting the threat landscape and the associated mitigation techniques in a single comparative matrix, we identify the areas where further research can be particularly beneficial."
2007,Fulbright2007,"The last fifteen years has seen a rise in interest in a body of work called TRIZ (pronounced trees) comprising a set of tools, methodologies, knowledge bases, and cognitive approaches designed to turn creative and innovative thinking into an algorithmic exercise. Since TRIZ is new to the computer security domain, this paper gives a brief summary of classical TRIZ dating back to 1946 and continues to present day in introducing one of the contemporary derivatives of TRIZ called I-TRIZ. Principal ITRIZ components are discussed and we show how to use I-TRIZ in two problems involving issues in computer security.
"
2003,Alpcan2003,"We investigate the basic trade-offs, analysis and decision processes involved in information security and intrusion detection, as well as possible application of game theoretic concepts to develop a formal decision and control framework. A generic model of a distributed intrusion detection system (IDS) with a network of sensors is considered, and two schemes based on game theoretic techniques are proposed. The security warning system is simple and easy-to-implement, and it gives system administrators an intuitive overview of the security situation in the network. The security attack game, on the other hand, models and analyzes attacker and IDS behavior within a two-person, nonzero-sum, noncooperative game with dynamic information. Nash equilibrium solutions in closed form are obtained for specific subgames, and two illustrative examples are provided.
"
2006,Maloof2006,"Central to the approaches described in this volume is the use of algorithms to build models from data. Depending on the algorithm, the model, or the data, we might call such an activity pattern classification [16, 17], statistical pattern recognition [18, 19], information retrieval [20], machine learning [2, 21, 22], data mining [3, 23, 24], or statistical learning [25]. Although finding the boundaries between concepts is important in all of these endeavors, in this chapter, we will instead focus on their commonalities. Indeed, there are methods common to all of these disciplines, and methods from all have been applied to problems in computer security.

Researchers and practitioners apply such algorithms to data for two main reasons: to predict new data and to better understand existing data. Regarding the former reason, one gathers data, applies an algorithm, and uses the resulting model to predict something about new data. For instance, we may want to predict based on audit data if a user’s current session is similar to old ones.

Regarding the second reason, one gathers data, applies an algorithm, and analyzes the resulting model to gain insights into the data that would be difficult to ascertain if examining only the data itself. For example, by analyzing a model derived from audit data, we might conclude that a particular person’s CPU usage is far higher than that of others, which might suggest inappropriate usage of computing resources. In this scenario, the need to understand the model an algorithm produces restricts the use of some algorithms, for some algorithms produce models that are easily understood, while others do not."
2015,Willard2015,"This article examines the notion of cyberattack-and-defend co-evolution as a mechanism to better understand the influences that the opposing forces have on each other. The concept of co-evolution has been most commonly applied to a biological context involving living organisms and nature-based adaptations, but it can be applied to technological domains as well. Cybersecurity strategies based solely on technological features of attack-and-defend adaptations do not immediately reveal a co-evolutionary relationship and are typically seen more as cyber arms races. In order to leverage cyber co-evolution in support of cybersecurity, the human-driven behaviors of cyberattack-and-defend adaptations have to be incorporated. In other words, the mission must serve to drive human motives and goals, and in many cases, must limit the scope of an attacker's adaptations.
"
2015,Biryukov2015,"This paper presents an aggregated model for the design of preemptive scenarios, based on the key principles of bio system memory. In recent years, special efforts have been made to create methods and tools that could effectively sup port mechanisms to protect critically important distributed information segments (CIDIS) from various information technology interventions (cyber attacks). It is proved in [1] that tools to counter computer attacks must detect and neutralize attacks within the minimum time (seconds, minutes) in order to allow for time to recover CIDIS (between several dozen minutes and several hours), and maintain stability at a given level. The proposed approach is valid. How ever, the most appropriate (desired) solution is one in which the potential impact of information technology on CIDIS is mitigated before it can produce any destructive effect on a protected resource. This is only possible if the information security system applied to protect CIDIS have properties similar to those of living organisms, which allow them to predict the future and perform certain actions to achieve goals in a changing world.
"
2017,Albanese2017,"In the last several decades, networked systems have grown in complexity and sophistication, introducing complex interdependencies amongst their numerous and diverse components. Attackers can leverage such interdependencies to penetrate seemingly well-guarded networks through sophisticated multi-step attacks. Research has shown that explicit and implicit interdependencies exist at various layers of the hardware and software architecture. In particular, dependencies between vulnerabilities and dependencies between applications and services are critical for assessing the impact of multi-step attacks. These two classes of interdependencies have been traditionally studied using attack and dependency graphs respectively. Although significant work has been done in the area of both attack and dependency graphs, we demonstrate that neither of these models can provide an accurate assessment of an attack's impact, when used in isolation. To address this limitation, we take a mission-centric approach and present a solution to integrate these two powerful models into a unified framework that enables us to accurately assess the impact of multi-step attacks and identify high-impact attack paths within a network. This analysis can ultimately generate effective hardening recommendations, and can be seen as one phase of a continuous process that iteratively cycles through impact analysis and vulnerability remediation stages.
"
2015,Bradshaw2015,"In this article we describe how we apply the concept of coactive emergence as a phenomenon of complexity that has implications for the design of sensemaking support tools involving a combination of human analysts and software agents. We apply this concept in the design of work methods for distributed sensemaking in cybersecurity work. Sensemaking is a motivated, continuous effort to understand, anticipate, and act upon complex situations. We discuss selected results of a macrocognitive work analysis that informed our focus for design and development of support tools. In that analysis, we identified seven target topics that would be the focus of our research: engaging automation as a full partner, reducing the volume of uncorrelated events, continuous knowledge discovery, more effective visualizations, collaboration and sharing, minimizing tedious work, and architecting scalability and resilience. In addressing the first target topic, we show how coactive emergence inspires an agent-supported threat understanding process that is consistent with Klein's Data/Frame theory of sensemaking. In subsequent sections, we describe our efforts to address the remaining six target topics as part of design and development of a cyber operations framework called Sol. Specifically, we describe the use of agents, policies, and visualization to enable coactive emergence for taskwork and teamwork. We also show how policy-governed agents working collaboratively with people can help in additional ways. We introduce the primary implementation frameworks that provide the core capabilities of our Sol cyber framework: the Luna Software Agent Framework and the KAoS Policy Services Framework. We describe results of initial studies addressing some of the issues raised in this article. Finally, we describe the status of Sol and plans for future development and evaluation.
"
2016,Razaq2016,"We present a novel Cyber Security analytics framework. We demonstrate a comprehensive cyber security monitoring system to construct cyber security correlated events with feature selection to anticipate behaviour based on various sensors.
"
2017,Bryant2017,"Network security investigations pose many challenges to security analysts attempting to identify the root cause of security alarms or incidents. Analysts are often presented with cases where either incomplete information is present, or an overwhelming amount of information is presented in a disorganized manner. Either scenario greatly impacts the ability for incident responders to properly identify and react to security incidents when they occur. The framework presented in this paper draws upon previous research pertaining to cyber threat modeling with kill-chains, as well as the practical application of threat modeling to forensic. Modifications were made to conventional kill-chain models to facilitate logical data aggregation within a relational database collecting data across disparate remote sensors resulting in more detailed alarms to security analysts. The framework developed in this paper proved effective in identifying the relationship of security alarms along a continuum of expected behaviors conducive to executing security investigations in a methodical manner. This framework effectively addressed incomplete or inadequate alarm information through aggregation, and provided a methodology for organizing related data and conducting standard investigations. Both improvements proved instrumental in the effective identification of security threats in a more expeditious manner.
"
2017,Aleroud2017,"Phishing has become an increasing threat in online space, largely driven by the evolving web, mobile, and social networking technologies. Previous phishing taxonomies have mainly focused on the underlying mechanisms of phishing but ignored the emerging attacking techniques, targeted environments, and countermeasures for mitigating new phishing types. This survey investigates phishing attacks and anti-phishing techniques developed not only in traditional environments such as e-mails and websites, but also in new environments such as mobile and social networking sites. Taking an integrated view of phishing, we propose a taxonomy that involves attacking techniques, countermeasures, targeted environments and communication media. The taxonomy will not only provide guidance for the design of effective techniques for phishing detection and prevention in various types of environments, but also facilitate practitioners in evaluating and selecting tools, methods, and features for handling specific types of phishing problems.
"
2015,Grini2015Thesis,"There exist different methods of identifying malware, and widespread method is the one found in almost every antivirus solution on the market today; the signature based approach. This approach uses a one-way cryptographic function to generate a unique hash of each file. Afterwards, each hash is checked against a database of hashes of known malware. This method provides close to none false positives, but this does also mean that this approach can only detect previously known malware, and will in many cases also provide a number of false negatives. Malware authors exploit this weakness in the way that they change a small part of the malicious code, and thereby changes the entire hash of the file, which then leaves the malicious code undetectable until the sample is discovered, analyzed and updated in the vendors database(s). In the light of this relatively easy mitigation for malware authors, it is clear that we need other ways to identify malware. The other two main approaches for this are static analysis and behavior based/dynamic analysis. The primary goal of such analysis and previous research has been focused around detecting whether a file is malicious or benign (binary classification). There has been comprehensive work in these fields the last few years. In the work we are proposing, we will leverage results from static analysis using machine learning methods, to distinguish malicious Windows executables. Not just benign/malicious as in many researches, but by malware family affiliation. To do this we will use a database consisting of about of 330.000 malicious executables. A challenge in this work will be the naming of the samples and families as different antivirus vendors labels samples with different names and follows no standard naming scheme. This is exemplified by e.g. the VirusTotal online scanner which scans a hash in 57 malware databases. For the static analysis we will use the VirusTotal scanner as well as an open source tool for analyzing portable executables, PEframe. The work performed in the thesis presents a novel approach to extract and construct features that can be used to make an estimation of which type and family a malicious file is an instance of, which can be useful for analysis and antivirus scanners. This contribution is novel because multinominal classification is applied to distinguish between different types and families."
2003,Ruhe2003,"Anticipating failures is a powerful tool for shortening the time required to develop robust products. Inverting Standard Techniques of Substance-Field analysis allows anticipation of failures caused by a useful function not being performed. “Value can only be defined by the ultimate customer. And it's only meaningful when expressed in terms of a specific product . . . which meets the customer's needs at the right time at the right price. Value is created by producers - from the customer standpoint, this is why producers exist.” In this age of global competition and Internet marketplaces, the only certainty for producers is that customers define - and rapidly redefine - value. The producer with the product that best meets the customer needs first has market advantage. Producers commonly model customer and producer value as having three aspects: Price Cost Customer Value Model
"
2017,Bullough2017,"Each year, thousands of software vulnerabilities are discovered and reported to the public. Unpatched known vulnerabilities are a significant security risk. It is imperative that software vendors quickly provide patches once vulnerabilities are known and users quickly install those patches as soon as they are available. However, most vulnerabilities are never actually exploited. Since writing, testing, and installing software patches can involve considerable resources, it would be desirable to prioritize the remediation of vulnerabilities that are likely to be exploited. Several published research studies have reported moderate success in applying machine learning techniques to the task of predicting whether a vulnerability will be exploited. These approaches typically use features derived from vulnerability databases (such as the summary text describing the vulnerability) or social media posts that mention the vulnerability by name. However, these prior studies share multiple methodological shortcomings that inate predictive power of these approaches. We replicate key portions of the prior work, compare their approaches, and show how selection of training and test data critically affect the estimated performance of predictive models. The results of this study point to important methodological considerations that should be taken into account so that results reflect real-world utility.
"
2011,DangPham2011,"While prior research has been examining information security behaviours in mature environments with formal policies and practices, there is less attention paid to new or transforming environments that lack security controls. It is crucial to understand what factors affect the formation of an emerging information security environment, so that security managers can make use of the forming mechanisms to improve the security environment without relying too much on enforcement. This research adopts exponential random graph modelling to predict the occurrence of information security influence among 114 employees in a recently established construction organisation. Our empirical findings show that physically co-locating, as well as having specific senior levels and similar tenure can result in more security influence. Other contributing work relationships include the exchange of workrelated advice, interpersonal trust, and seeing others as role model and long-term collaborators. The structural features of the information security influence network were also examined, which offer strategies for security managers to diffuse security behaviours within the workplace. Furthermore, specific directions for future network research were elaborated in detail.
"
2016,LiPaja2016,"Discovering potential attacks on a system is an essential step in engineering secure systems, as the identified attacks will determine essential security requirements. The prevalence of Socio-Technical Systems (STSs) makes attack analysis particularly challenging. These systems are composed of people and organizations, their software systems, as well as physical infrastructures. As such, a thorough attack analysis needs to consider strategic (social and organizational) aspects of the involved people and organizations, as well as technical aspects affecting software systems and the physical infrastructure, requiring a large amount of security knowledge which is difficult to acquire. In this paper, we propose a systematic approach to efficiently leverage a comprehensive attack knowledge repository (CAPEC) in order to identify realistic and detailed attack behaviors, avoiding severe repercussions of security breaches. In particular, we propose a systematic method to model CAPEC attack patterns, which has been applied to 102 patterns, in order to semi-automatically select and apply such patterns. Using the CAPEC patterns as part of a systematic and tool-supported process, we can efficiently operationalize attack strategies and identify realistic alternative attacks on an STS. We validate our proposal by performing a case study on a smart grid scenario. "
2001,Kaplan2001,"A point of view is suggested from which the Hierarchical Holographic Modeling (HHM) method can be seen as one more method within the Theory of Scenario Structuring (TSS), which is that part of Quantitative Risk Assessment having to do with the task of identifying the set of risk scenarios. Seen in this way, HHM brings strongly to our attention the fact that different methods within TSS can result in different sets of risk scenarios for the same underlying problem. Although this is not a problem practically, it is a bit awkward conceptually from the standpoint of the “set of triplets” definition of risk, in which the scenario set is part of the definition. Accordingly, the present article suggests a refinement to the set of triplets definition, which removes the specific set of scenarios, found by any of the TSS methods, from the definition of risk and casts it, instead, as an approximation to the “true” set of scenarios that is native to the problem at hand and not affected by the TSS method used.
"
2016,Samtani2016b,"Cyber threats pose grave national security dangers to the US. Many cyber-attacks today are executed with evergrowing collection of malicious tools. Cyber threat intelligence (CTI) and malware analysis portals aim to provide knowledge and tools to help prevent and mitigate attacks. However, current CTI and malware analysis portals and techniques have been criticized for being too reactive as they rely on data collected from past cyber-attacks. Online hacker forums provide a novel source of data that can inform a proactive CTI and malware portal. This research demonstrates the AZSecure Hacker Assets Portal. This portal collects and analyzes malicious assets directly from the largely untapped and rich data source of online hacker communities by utilizing state-of-the-art machine learning techniques. This paper explores the development and evolution of the AZSecure Hacker Assets Portal. We also present key portal functionalities such as asset searching, browsing, and downloading, source code visualizations and code comparison analytics, and an interactive CTI dashboard.
"
2009,Chowdury2009,"Software security failures are common and the problem is growing. A vulnerability is a weakness in the software that, when exploited, causes a security failure. It is difficult to detect vulnerabilities until they manifest themselves as security failures in the operational stage of the software, because security concerns are often not addressed or known sufficiently early during the Software Development Life Cycle (SDLC). Complexity, coupling, and cohesion (CCC) related software metrics can be measured during the early phases of software development such as design or coding. Although these metrics have been successfully employed to indicate software faults in general, the relationships between CCC metrics and vulnerabilities have not been extensively investigated yet. If empirical relationships can be discovered between CCC metrics and vulnerabilities, these metrics could aid software developers to take proactive actions against potential vulnerabilities in software.
In this thesis, we investigate whether CCC metrics can be utilized as early indicators of software vulnerabilities. We conduct an extensive case study on several releases of Mozilla Firefox to provide empirical evidence on how vulnerabilities are related to complexity, coupling, and cohesion. We mine the vulnerability databases, bug databases, and version archives of Mozilla Firefox to map vulnerabilities to software entities. It is found that some of the CCC metrics are correlated to vulnerabilities at a statistically significant level. Since different metrics are available at different development phases, we further examine the correlations to determine which level (design or code) of CCC metrics are better indicators of vulnerabilities. We also observe that the correlation patterns are stable across multiple releases. These observations imply that the metrics can be dependably used as early indicators of vulnerabilities in software.
We then present a framework to automatically predict vulnerabilities based on CCC metrics. To build vulnerability predictors, we consider four alternative data mining and statistical techniques – C4.5 Decision Tree, Random Forests, Logistic Regression, and Naïve-Bayes – and compare their prediction performances. We are able to predict majority of the vulnerability-prone files in Mozilla Firefox, with tolerable false positive rates. Moreover, the predictors built from the past releases can reliably predict the likelihood of having vulnerabilities in future releases. The experimental results indicate that structural information from the non-security realm such as complexity, coupling, and cohesion are useful in vulnerability prediction."
2010,Nguyen2010b,"Security metrics and vulnerability prediction for software have gained a lot of interests from the community. Many software security metrics have been proposed e.g., complexity metrics, cohesion and coupling metrics. In this paper, we propose a novel code metric based on dependency graphs to predict vulnerable components. To validate the e ciency of the proposed metric, we conduct a prediction model which targets the JavaScript Engine of Firefox. In this experiment, our prediction model has obtained a very good result in term of accuracy and recall rates. This empirical result is a good evidence showing dependency graphs are also a good option for early indicating vulnerability.
"
2008,Weick2008,"High Reliability Organizations (HROs) have been considered outliers in terms of organizational theory due to their unique potentials for catastrophic consequences and interactively complex technology. The authors contend that HROs are more central to the mainstream because they provide a unique window into organizational effectiveness under trying circumstances. 
Effective HROs organize socially around failure rather than success in ways that induce an ongoing state of mindfulness. Mindfulness, in turn, facilitates the discovery and correction of anomalies that could cumulate with other anomalies and grow into a catastrophe. Mindfulness, with its rich awareness of discriminatory detail, enables people to manage juxtapositions of events they have never seen before. But the ways in which they do this are still not fully understood. Our analysis represents an effort to further this understanding.
Effective HROs represent complex adaptive systems that combine orderly processes of cognition with variations in routine activities in order to sense and manage complex ill-structured contingencies. In a dynamic, unknowable, unpredictable world one might presume that organizing in a manner analogous to HROs would be in the best interest of most organizations. Hints of such moves are evident when traditional organizations graft TQM cultures onto a pre-existing preoccupation with efficiency, and aspire to the relatively error-free performance found in HROs. But many of these attempted changes fail because traditional organizations demonstrate little awareness of just what kind of infrastructure it takes to support reliable performance. Unfortunately, mainstream organizational theory isn’t much help in developing this awareness.
The purpose of our analysis has been to consolidate conceptually a body of work that begins to articulate the social infrastructure of reliability. The language of a near miss, having the bubble, migrating decisions, conceptual slack, resilience, normal accidents, redundancy, variable disjunction, struggle for alertness, performance pressure, situational awareness, interactive complexity, and prideful wariness, describes how people organize around failures in ways that induce mindful awareness. That mindfulness, in turn, reveals unexpected threats to well being that can escalate out of control. And that, in our estimation, is a central theme for mainstream organizational theory."
2016,Mouton2016,"The field of information security is a fast-growing discipline. Even though the effectiveness of security measures to protect sensitive information is increasing, people remain susceptible to manipulation and thus the human element remains a weak link. A social engineering attack targets this weakness by using various manipulation techniques to elicit sensitive information. The field of social engineering is still in its early stages with regard to formal definitions, attack frameworks and templates of attacks. This paper proposes detailed social engineering attack templates that are derived from real-world social engineering examples. Current documented examples of social engineering attacks do not include all the attack steps and phases. The proposed social engineering attack templates attempt to alleviate the problem of limited documented literature on social engineering attacks by mapping the real-world examples to the social engineering attack framework. Mapping several similar real-world examples to the social engineering attack framework allows one to establish a detailed flow of the attack whilst abstracting subjects and objects. This mapping is then utilised to propose the generalised social engineering attack templates that are representative of real-world examples, whilst still being general enough to encompass several different real-world examples. The proposed social engineering attack templates cover all three types of communication, namely bidirectional communication, unidirectional communication and indirect communication. In order to perform comparative studies of different social engineering models, processes and frameworks, it is necessary to have a formalised set of social engineering attack scenarios that are fully detailed in every phase and step of the process. The social engineering attack templates are converted to social engineering attack scenarios by populating the template with both subjects and objects from real-world examples whilst still maintaining the detailed flow of the attack as provided in the template. Furthermore, this paper illustrates how the social engineering attack scenarios are applied to verify a social engineering attack detection model. These templates and scenarios can be used by other researchers to either expand on, use for comparative measures, create additional examples or evaluate models for completeness. Additionally, the proposed social engineering attack templates can also be used to develop social engineering awareness material.
"
2016,Lazka2016,"In recent years, we have seen a number of successful attacks against high-profile targets, some of which have even caused severe physical damage. These examples have shown us that resourceful and determined attackers can penetrate virtually any system, even those that are secured by the ""air-gap."" Consequently, in order to minimize the impact of stealthy attacks, defenders have to focus not only on strengthening the first lines of defense but also on deploying effective intrusion detection systems. Intrusion-detection systems can play a key role in protecting sensitive computer systems since they give defenders a chance to detect and mitigate attacks before they could cause substantial losses. However, an oversensitive intrusion-detection system, which produces a large number of false alarms, imposes prohibitively high operational costs on a defender since alarms need to be manually investigated. Thus, defenders have to strike the right balance between maximizing security and minimizing costs. Optimizing the sensitivity of intrusion detection systems is especially challenging in the case when multiple interdependent computer systems have to be defended against a strategic attacker, who can target computer systems in order to maximize losses and minimize the probability of detection. We model this scenario as an attacker-defender security game and study the problem of finding optimal intrusion detection thresholds.
"
2017,Cook2017,"The threat to Industrial Control Systems (ICS) from cyber attacks is widely acknowledged by governments and literature. Operators of ICS are looking to address these threats in an effective and cost-sensitive manner that does not expose their operations to additional risks through invasive testing. Whilst existing standards and guidelines offer comprehensive advice for reviewing the security of ICS infrastructure, resource and time limitations can lead to incomplete assessments or undesirably long countermeasure implementation schedules. In this paper we consider the problem of undertaking efficient cyber security risk assessments and implementing mitigations in large, established ICS operations for which a full security review cannot be implemented on a constrained timescale. The contribution is the Industrial Control System Cyber Defence Triage Process (ICS-CDTP). ICS-CDTP determines areas of priority where the impact of attacks is greatest, and where initial investment reduces the organisation's overall exposure swiftly. ICS-CDTP is designed to be a precursor to a wider, holistic review across the operation following established security management approaches. ICS-CDTP is a novel combination of the Diamond Model of Intrusion Analysis, the Mandiant Attack Lifecycle, and the CARVER Matrix, allowing for an effective triage of attack vectors and likely targets for a capable antagonist. ICS-CDTP identifies and focuses on key ICS processes and their exposure to cyber threats with the view to maintain critical operations. The article defines ICS-CDTP and exemplifies its application using a fictitious water treatment facility, and explains its evaluation as part of a large-scale serious game exercise.
"
2010,Barreno2010,"Machine learning's ability to rapidly evolve to changing and complex situations has helped it become a fundamental tool for computer security. That adaptability is also a vulnerability: attackers can exploit machine learning systems. We present a taxonomy identifying and analyzing attacks against machine learning systems. We show how these classes influence the costs for the attacker and defender, and we give a formal structure defining their interaction. We use our framework to survey and analyze the literature of attacks against machine learning systems. We also illustrate our taxonomy by showing how it can guide attacks against SpamBayes, a popular statistical spam filter. Finally, we discuss how our taxonomy suggests new lines of defenses."
2014,Chapman2014,"In order to begin to solve many of the problems in the domain of cyber security, they must first be transformed into abstract representations, free of complexity and paralysing technical detail. We believe that for many classic security problems, a viable transformation is to consider them as an abstract game of hide-and-seek. The tools required in this game - such as strategic search and an appreciation of an opponent's likely strategies - are very similar to the tools required in a number of cyber security applications, and thus developments in strategies for this game can certainly benefit the domain. In this paper we consider hide-and-seek as a formal game, and consider in depth how it is allegorical to the cyber domain, particularly in the problems of attack attribution and attack pivoting. Using this as motivation, we consider the relative performance of several hide and seek strategies using an agent-based simulation model, and present our findings as an initial insight into how to proceed with the solution of real cyber issues.
"
2017,Choi2017,"This study examines trends in academic research on personal information privacy. Using Scopus DB, we extracted 2356 documents covering journal articles, reviews, book chapters, conference papers, and working papers published between 1972 and August 2015. Latent Dirichlet allocation (LDA) is applied to the abstracts of those extracted documents to identify topics. Topics discovered from all documents focus mainly on technology, and the findings indicate that algorithms, Facebook privacy, and online social networks have become prominent topics. In contrast, it was observed that journal articles put more emphasis on both the e-business and healthcare. These results identify a research gap in the area of personal information privacy and offer a direction for future research."
2013,Shuai2013,"In order to solve the problems of traditional machine learning methods for automatic classification of vulnerability, this paper presents a novel machine learning method based on LDA model and SVM. Firstly, word location information is introduced into LDA model called WL-LDA (Weighted Location LDA), which could acquire better effect through generating vector space on themes other than on words. Secondly, a multi-class classifier called HT-SVM (Huffman Tree SVM) is constructed, which could make a faster and more stable classification by making good use of the prior knowledge about distribution of the number of vulnerabilities. Experiments show that the method could obtain higher classification accuracy and efficiency.
"
2011,Jajodia2011,"The cyber situational awareness of an organization determines its effectiveness in responding to attacks. Mission success is highly dependent on the availability and correct operation of complex computer networks, which are vulnerable to various types of attacks. Today, situational awareness capabilities are limited in many ways, such as inaccurate and incomplete vulnerability analysis, failure to adapt to evolving networks and attacks, inability to transform raw data into cyber intelligence, and inability for handling uncertainty. We describe advanced capabilities for mission-centric cyber situational awareness, based on defense in depth, provided by the Cauldron tool. Cauldron automatically maps all paths of vulnerability through networks, by correlating, aggregating, normalizing, and fusing data from a variety of sources. It provides sophisticated visualization of attack paths, with automatically generated mitigation recommendations. Flexible modeling supports multistep analysis of firewall rules as well as host-to-host vulnerability, with attack vectors inside the network as well as from the outside. We describe alert correlation based on Caldron attack graphs, along with analysis of mission impact from attacks.
"
2016,Balzacq2016,"This article argues that some core tenets of Actor-Network Theory (ANT) can serve as heuristics for a better understanding of what the stakes of cyber-security are, how it operates, and how it fails. Despite the centrality of cyber-incidents in the cyber-security discourse, researchers have yet to understand their link to, and affects on politics. We close this gap by combining ANT insights with an empirical examination of a prominent cyber-incident (Stuxnet). We demonstrate that the disruptive practices of cyber-security caused by malicious software (malware), lie in their ability to actively perform three kinds of space (regions, networks, and fluids), each activating different types of political interventions. The article posits that the fluidity of malware challenges the consistency of networks and the sovereign boundaries set by regions, and paradoxically, leads to a forceful re-enactment of them. In this respect, the conceptualisation of fluidity as an overarching threat accounts for multiple policy responses and practices in cyber-security as well as attempts to (re-)establish territoriality and borders in the virtual realm. While this article concentrates on cyber-security, its underlying ambition is to indicate concretely how scholars can profitably engage ANT's concepts and methodologies.
"
2017,Eibl2017,"Process mining is a set of data mining techniques that learn and analyze processes based on event logs. While process mining has recently been proposed for intrusion detection in business processes, it has never been applied to smart metering processes. The goal of this paper is to explore the potential of process mining for the detection of intrusions into smart metering systems. As a case study the remote shutdown process has been modeled and a threat analysis was conducted leading to an extensive attack tree. It is shown that currently proposed process mining techniques based on conformance checking do not suffice to find all attacks of the attack tree; an inclusion of additional perspectives is necessary. Consequences for the design of a realistic testing environment based on simulations are discussed.
"
2003,Guyon2003,"Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.
"
2017,Veeramachaneni2017,"We present an analyst-in-the-loop security system, where analyst intuition is put together with stateof-the-art machine learning to build an end-to-end active learning system. The system has four key features: a big data behavioral analytics platform, an ensemble of outlier detection methods, a mechanism to obtain feedback from security analysts, and a supervised learning module. When these four components are run in conjunction on a daily basis and are compared to an unsupervised outlier detection method, detection rate improves by an average of 3.41 , and false positives are reduced fivefold. We validate our system with a real-world data set consisting of 3.6 billion log lines. These results show that our system is capable of learning to defend against unseen attacks.
"
